{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "5kPSC78hAsiU",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-16 01:02:52.739565: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-07-16 01:02:52.773288: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-16 01:02:52.798914: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-16 01:02:52.799068: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-16 01:02:53.126379: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-16 01:02:53.126522: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-16 01:02:53.126628: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-16 01:02:53.126715: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /device:GPU:0 with 5665 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070, pci bus id: 0000:2b:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 4011170413003185847\n",
       " xla_global_id: -1,\n",
       " name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 5940510720\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 1471383505664701861\n",
       " physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 3070, pci bus id: 0000:2b:00.0, compute capability: 8.6\"\n",
       " xla_global_id: 416903419]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape is: (92, 69, 100, 100, 6)\n",
      "Splitted trainning dataset shape is: (69, 69, 100, 100, 6)\n",
      "Splitted trainning dataset shape is: (23, 69, 100, 100, 6)\n"
     ]
    }
   ],
   "source": [
    "# import sys\n",
    "# log_path = 'log.txt'\n",
    "# sys.stdout = open(log_path, \"w\")\n",
    "\n",
    "import h5py\n",
    "h5 = h5py.File('./data/test_5_1_07112022.hdf5', mode='r')\n",
    "dataset = h5.get('phi')\n",
    "\n",
    "data_sz = dataset.shape[0]\n",
    "indexes = np.arange(data_sz)\n",
    "np.random.shuffle(indexes)\n",
    "train_index = np.sort(indexes[: int(0.75 * data_sz)])\n",
    "val_index = np.sort(indexes[int(0.75 * data_sz) :])\n",
    "\n",
    "print(f'Dataset shape is: {dataset.shape}')\n",
    "train_dataset = np.take(dataset,train_index,axis=0)\n",
    "val_dataset = np.take(dataset,val_index,axis=0)\n",
    "print(f'Splitted trainning dataset shape is: {train_dataset.shape}')\n",
    "print(f'Splitted trainning dataset shape is: {val_dataset.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ...: 100%|██████████| 69/69 [00:03<00:00, 20.61it/s]\n",
      "Running ...: 100%|██████████| 23/23 [00:01<00:00, 20.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Shapes: (4416, 5, 100, 100, 5), (4416, 2, 100, 100, 5)\n",
      "Validation Dataset Shapes: (1472, 5, 100, 100, 5), (1472, 2, 100, 100, 5)\n",
      "Training Dataset Shapes: (3000, 5, 100, 100, 5), (3000, 2, 100, 100, 5)\n",
      "Validation Dataset Shapes: (1000, 5, 100, 100, 5), (1000, 2, 100, 100, 5)\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def create_shifted_frames_5to1(dataset):\n",
    "    numFrames = 5\n",
    "    sz = dataset.shape[0]*(dataset.shape[1]-5)\n",
    "    x = np.zeros([sz, numFrames, 100, 100, 5])\n",
    "    y = np.zeros([sz, 2, 100, 100, 5])\n",
    "    k = 0\n",
    "    for i in tqdm(range(dataset.shape[0]), desc=\"Running ...\"):\n",
    "        for j in range(dataset.shape[1]-numFrames):\n",
    "            currentFrame = np.expand_dims(np.expand_dims(dataset[i,j+numFrames,:,:,0:5],axis=0),axis=0)\n",
    "            pastFrames = np.expand_dims(dataset[i,j:(j+numFrames),:,:,0:5],axis=0)\n",
    "            x[k,...] = pastFrames\n",
    "            y[k,0,...] = currentFrame\n",
    "            y[k,1,...] = pastFrames[:,-1,...] # passing past frame to ground truth for use in loss function (an issue with directly passing inp into loss func)\n",
    "            k+=1  \n",
    "    return x,y\n",
    "\n",
    "x_train, y_train = create_shifted_frames_5to1(train_dataset)\n",
    "x_val, y_val = create_shifted_frames_5to1(val_dataset)\n",
    "\n",
    "x_train[...,2] = x_train[...,2]*10\n",
    "y_train[...,2] = y_train[...,2]*10\n",
    "x_val[...,2] = x_val[...,2]*10\n",
    "y_val[...,2] = y_val[...,2]*10\n",
    "\n",
    "print(\"Training Dataset Shapes: \" + str(x_train.shape) + \", \" + str(y_train.shape))\n",
    "print(\"Validation Dataset Shapes: \" + str(x_val.shape) + \", \" + str(y_val.shape))\n",
    "\n",
    "# some wierd memory issue with tensorflow, same batch size with smaller dataset size works\n",
    "x_train = np.take(x_train,np.arange(3000),axis=0)\n",
    "y_train = np.take(y_train,np.arange(3000),axis=0)\n",
    "x_val = np.take(x_val,np.arange(1000),axis=0)\n",
    "y_val = np.take(y_val,np.arange(1000),axis=0)\n",
    "\n",
    "print(\"Training Dataset Shapes: \" + str(x_train.shape) + \", \" + str(y_train.shape))\n",
    "print(\"Validation Dataset Shapes: \" + str(x_val.shape) + \", \" + str(y_val.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 5, 100, 100, 5)\n",
      "1098\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Frame 5')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x240 with 12 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "data_choice = np.random.choice(range(len(x_train)), size=1)[0]\n",
    "print(data_choice)\n",
    "channel = 0\n",
    "plt.figure(figsize=(20, 4), dpi=60)\n",
    "for i in range(5):\n",
    "    plt.subplot(1,6,i+1)\n",
    "    plt.imshow(x_train[data_choice,i,:,:,channel])\n",
    "    plt.colorbar()\n",
    "    plt.title(f\"Frame {i}\")\n",
    "\n",
    "plt.subplot(1,6,6)\n",
    "plt.imshow(y_train[data_choice,0,:,:,channel])\n",
    "plt.colorbar()\n",
    "plt.title(\"Frame 5\")\n",
    "\n",
    "# plt.subplot(1,6,1)\n",
    "# plt.imshow(x_train[data_choice,0,:,:,channel])\n",
    "# plt.colorbar()\n",
    "# plt.title(\"Frame 0\")\n",
    "# plt.subplot(1,6,2)\n",
    "# plt.imshow(x_train[data_choice,1,:,:,channel])\n",
    "# plt.colorbar()\n",
    "# plt.title(\"Frame 1\")\n",
    "# plt.subplot(1,6,3)\n",
    "# plt.imshow(x_train[data_choice,2,:,:,channel])\n",
    "# plt.colorbar()\n",
    "# plt.title(\"Frame 2\")\n",
    "# plt.subplot(1,6,4)\n",
    "# plt.imshow(x_train[data_choice,3,:,:,channel])\n",
    "# plt.colorbar()\n",
    "# plt.title(\"Frame 3\")\n",
    "# plt.subplot(1,6,5)\n",
    "# plt.imshow(x_train[data_choice,4,:,:,channel])\n",
    "# plt.colorbar()\n",
    "# plt.title(\"Frame 4\")\n",
    "# plt.subplot(1,6,6)\n",
    "# plt.imshow(y_train[data_choice,0,:,:,channel])\n",
    "# plt.colorbar()\n",
    "# plt.title(\"Frame 5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_deriv(input):\n",
    "    N1N_input, NN1_input = tf.image.image_gradients(input)\n",
    "    N2N_input, _ = tf.image.image_gradients(N1N_input)\n",
    "    _, NN2_input = tf.image.image_gradients(NN1_input)\n",
    "    LAP_input = tf.math.add(N2N_input,NN2_input)\n",
    "    return N1N_input, NN1_input, N2N_input, NN2_input, LAP_input\n",
    "\n",
    "def get_MSE_tf(input,goal):\n",
    "    return tf.math.reduce_mean(tf.square(tf.math.subtract(input, goal)))\n",
    "\n",
    "def get_MRE_tf(input,goal):\n",
    "    error = 1\n",
    "    return error\n",
    "    \n",
    "def PINN_loss():\n",
    "    def loss(y_true, y_pred):\n",
    "        dt = 0.01*500 # current data sampled per 500 iter (change based on dataset)\n",
    "        \n",
    "        true_phi = tf.cast(tf.expand_dims(tf.expand_dims(y_true[0,0,:,:,0],axis=0),axis=3), dtype=tf.float64)\n",
    "        true_tub = tf.cast(tf.expand_dims(tf.expand_dims(y_true[0,0,:,:,2],axis=0),axis=3), dtype=tf.float64)\n",
    "        true_tempr = tf.cast(tf.expand_dims(tf.expand_dims(y_true[0,0,:,:,3],axis=0),axis=3), dtype=tf.float64)\n",
    "        theta = tf.cast(tf.expand_dims(tf.expand_dims(y_true[0,0,:,:,4],axis=0),axis=3), dtype=tf.float64) # Since theta is passed, theta remains the same\n",
    "\n",
    "        NN_p = tf.cast(tf.expand_dims(tf.expand_dims(y_true[0,1,:,:,0],axis=0),axis=3), dtype=tf.float64)\n",
    "        tub_p = tf.cast(tf.expand_dims(tf.expand_dims(y_true[0,1,:,:,2],axis=0),axis=3), dtype=tf.float64)\n",
    "        tempr_p = tf.cast(tf.expand_dims(tf.expand_dims(y_true[0,1,:,:,3],axis=0),axis=3), dtype=tf.float64)\n",
    "\n",
    "        NN_pK = tf.cast(tf.expand_dims(tf.expand_dims(y_pred[0,-1,:,:,0],axis=0),axis=3), dtype=tf.float64)\n",
    "        tips_pK = tf.cast(tf.expand_dims(tf.expand_dims(y_pred[0,-1,:,:,1],axis=0),axis=3), dtype=tf.float64)\n",
    "        tub_pK = tf.cast(tf.expand_dims(tf.expand_dims(y_pred[0,-1,:,:,2],axis=0),axis=3), dtype=tf.float64)\n",
    "        tempr_pK = tf.cast(tf.expand_dims(tf.expand_dims(y_pred[0,-1,:,:,3],axis=0),axis=3), dtype=tf.float64)\n",
    "\n",
    "        N1N_p, NN1_p, _, _, LAP_p = get_deriv(NN_p)\n",
    "        N1N_theta, NN1_theta, _, _, _ = get_deriv(theta)\n",
    "        _, _, _, _, LAP_tp = get_deriv(tempr_p)\n",
    "        N1N_tb, NN1_tb, _, _, LAP_tb = get_deriv(tub_p)\n",
    "\n",
    "        # Temperature residual\n",
    "        tempr_residual = (3*LAP_tp+4*(NN_pK - NN_p)/dt)*dt + tempr_p - tempr_pK\n",
    "\n",
    "        # Tubulin residual\n",
    "        diff_tb = 4*(tf.math.multiply(N1N_p,N1N_tb) + tf.math.multiply(NN1_p,NN1_tb) + tf.math.multiply(NN_p,LAP_tb))\n",
    "        alph_tb = 0.001*(tf.math.multiply(N1N_p,tub_p) + tf.math.multiply(NN_p,N1N_tb) + tf.math.multiply(NN1_p,tub_p) + tf.math.multiply(NN_p,NN1_tb))\n",
    "        beta_tb = 0.001*tf.math.multiply(NN_p,tub_p)\n",
    "        src_tb = tf.math.divide_no_nan(15*tf.math.square(LAP_p),tf.math.reduce_sum(tf.math.square(LAP_p)))\n",
    "        tub_residual = (diff_tb - alph_tb - beta_tb + src_tb)*dt + tub_p - tub_pK\n",
    "\n",
    "        # Energy calculation\n",
    "        e = 0.2865*tf.math.atan(10*tf.math.multiply(tf.math.multiply(tips_pK,5*tub_pK-0.1),1-tempr_pK))\n",
    "\n",
    "        # phase field residual\n",
    "        atheta = tf.math.atan2(NN1_p,N1N_p)\n",
    "        a = 0.04*(1.0+0.1*tf.math.cos(6*(atheta-theta)))\n",
    "        ap = -0.04*(6*0.1*tf.math.sin(6*(atheta-theta)))\n",
    "        aap = tf.math.multiply(a,ap)\n",
    "        a2 = tf.math.square(a)\n",
    "        a2Lap = tf.math.multiply(a2,LAP_p)\n",
    "        N1N_aapNN1p, _ = tf.image.image_gradients(tf.math.multiply(aap,NN1_p))\n",
    "        _, NN1_aapN1Np = tf.image.image_gradients(tf.math.multiply(aap,N1N_p))\n",
    "        mag_theta = tf.math.sqrt(tf.math.square(N1N_theta) + tf.math.square(NN1_theta))\n",
    "        dblwll_term = tf.math.multiply(tf.math.multiply(NN_p,1-NN_p),NN_p - 0.5 + e + 6*0.007*mag_theta)\n",
    "        phi_residual = a2Lap - N1N_aapNN1p + NN1_aapN1Np + dblwll_term - (NN_pK - NN_p) / dt\n",
    "\n",
    "        residual_mse = tf.math.reduce_mean(phi_residual) + tf.math.reduce_mean(tempr_residual) + tf.math.reduce_mean(tub_residual)\n",
    "        diff_mse = get_MSE_tf(NN_pK,true_phi) + get_MSE_tf(tempr_pK,true_tempr) + get_MSE_tf(tub_pK,true_tub)\n",
    "        loss = tf.math.abs(residual_mse) + tf.math.abs(diff_mse)\n",
    "        \n",
    "        return loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-16 01:03:23.898739: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-16 01:03:23.898916: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-16 01:03:23.899020: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-16 01:03:23.899266: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-16 01:03:23.899379: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-16 01:03:23.899481: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-16 01:03:23.899624: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-16 01:03:23.899736: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-16 01:03:23.899917: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5665 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070, pci bus id: 0000:2b:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "inp = layers.Input(shape=(x_train.shape[1:]))\n",
    "\n",
    "# Separable Convolution Encoding\n",
    "x0 = layers.TimeDistributed(layers.SeparableConv2D(\n",
    "    filters=64,\n",
    "    kernel_size=(5, 5),\n",
    "    strides = (2, 2),\n",
    "    depth_multiplier=1,\n",
    "    padding=\"valid\",\n",
    "    activation=\"relu\",\n",
    "))(inp)\n",
    "x1 = layers.TimeDistributed(layers.SeparableConv2D(\n",
    "    filters=128,\n",
    "    kernel_size=(5, 5),\n",
    "    strides = (2, 2),\n",
    "    depth_multiplier=1,\n",
    "    padding=\"valid\",\n",
    "    activation=\"relu\",\n",
    "))(x0)\n",
    "x2 = layers.ConvLSTM2D(\n",
    "    filters=128,\n",
    "    kernel_size=(5, 5),\n",
    "    padding=\"same\",\n",
    "    return_sequences=False,\n",
    "    activation=\"relu\",\n",
    "    # recurrent_dropout=drop_rate,\n",
    ")(x1)\n",
    "x2 = tf.expand_dims(x2,axis=1)\n",
    "\n",
    "# decoding for phi channel\n",
    "phi1 = x2\n",
    "phi2 = layers.TimeDistributed(layers.Conv2DTranspose(\n",
    "    filters=64,\n",
    "    kernel_size=(6, 6),\n",
    "    strides = (2, 2),\n",
    "    padding=\"valid\",\n",
    "    activation=\"sigmoid\",\n",
    "))(phi1)\n",
    "phi3 = layers.TimeDistributed(layers.Conv2DTranspose(\n",
    "    filters=1,\n",
    "    kernel_size=(6, 6),\n",
    "    strides = (2, 2),\n",
    "    padding=\"valid\",\n",
    "    activation=\"sigmoid\",\n",
    "))(phi2)\n",
    "\n",
    "# decoding for tips channel\n",
    "tip1 = phi3\n",
    "tip2 = layers.TimeDistributed(layers.Conv2DTranspose(\n",
    "    filters=1,\n",
    "    kernel_size=(6, 6),\n",
    "    strides = (1, 1),\n",
    "    padding=\"same\",\n",
    "    activation=\"sigmoid\",\n",
    "))(tip1)\n",
    "\n",
    "# decoding for tublin channel\n",
    "tub1 = x2\n",
    "tub2 = layers.TimeDistributed(layers.Conv2DTranspose(\n",
    "    filters=64,\n",
    "    kernel_size=(6, 6),\n",
    "    strides = (2, 2),\n",
    "    padding=\"valid\",\n",
    "    activation=\"relu\",\n",
    "))(tub1)\n",
    "tub3 = layers.TimeDistributed(layers.Conv2DTranspose(\n",
    "    filters=1,\n",
    "    kernel_size=(6, 6),\n",
    "    strides = (2, 2),\n",
    "    padding=\"valid\",\n",
    "    activation=\"relu\",\n",
    "))(tub2)\n",
    "tub4 = tf.math.multiply(phi3,tub3)\n",
    "\n",
    "# decoding for temperature channel\n",
    "phi_pre = tf.expand_dims(tf.expand_dims(inp[:,-1,:,:,0],axis=1),axis=4)\n",
    "phi_diff = phi3 - phi_pre\n",
    "tempr1 = x2\n",
    "tempr2 = layers.TimeDistributed(layers.Conv2DTranspose(\n",
    "    filters=64,\n",
    "    kernel_size=(6, 6),\n",
    "    strides = (2, 2),\n",
    "    padding=\"valid\",\n",
    "    activation=\"relu\",\n",
    "))(tempr1)\n",
    "tempr3 = layers.TimeDistributed(layers.Conv2DTranspose(\n",
    "    filters=1,\n",
    "    kernel_size=(6, 6),\n",
    "    strides = (2, 2),\n",
    "    padding=\"valid\",\n",
    "    activation=\"relu\",\n",
    "))(tempr2)\n",
    "tempr3 = tf.concat([tempr3,phi_diff],axis=4)\n",
    "tempr4 = layers.TimeDistributed(layers.Conv2DTranspose(\n",
    "    filters=1,\n",
    "    kernel_size=(6, 6),\n",
    "    strides = (1, 1),\n",
    "    padding=\"same\",\n",
    "    activation=\"relu\",\n",
    "))(tempr3)\n",
    "\n",
    "# passing theta (no change)\n",
    "theta1 = tf.expand_dims(tf.expand_dims(inp[:,0,:,:,-1],axis=1),axis=4)\n",
    "\n",
    "out = tf.concat([phi3,tip2,tub4,tempr4,theta1],axis=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 5, 100, 100  0           []                               \n",
      "                                , 5)]                                                             \n",
      "                                                                                                  \n",
      " time_distributed (TimeDistribu  (None, 5, 48, 48, 6  509        ['input_1[0][0]']                \n",
      " ted)                           4)                                                                \n",
      "                                                                                                  \n",
      " time_distributed_1 (TimeDistri  (None, 5, 22, 22, 1  9920       ['time_distributed[0][0]']       \n",
      " buted)                         28)                                                               \n",
      "                                                                                                  \n",
      " conv_lstm2d (ConvLSTM2D)       (None, 22, 22, 128)  3277312     ['time_distributed_1[0][0]']     \n",
      "                                                                                                  \n",
      " tf.expand_dims (TFOpLambda)    (None, 1, 22, 22, 1  0           ['conv_lstm2d[0][0]']            \n",
      "                                28)                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (Slic  (None, 100, 100)    0           ['input_1[0][0]']                \n",
      " ingOpLambda)                                                                                     \n",
      "                                                                                                  \n",
      " time_distributed_2 (TimeDistri  (None, 1, 48, 48, 6  294976     ['tf.expand_dims[0][0]']         \n",
      " buted)                         4)                                                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_1 (TFOpLambda)  (None, 1, 100, 100)  0           ['tf.__operators__.getitem[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " time_distributed_3 (TimeDistri  (None, 1, 100, 100,  2305       ['time_distributed_2[0][0]']     \n",
      " buted)                          1)                                                               \n",
      "                                                                                                  \n",
      " time_distributed_7 (TimeDistri  (None, 1, 48, 48, 6  294976     ['tf.expand_dims[0][0]']         \n",
      " buted)                         4)                                                                \n",
      "                                                                                                  \n",
      " tf.expand_dims_2 (TFOpLambda)  (None, 1, 100, 100,  0           ['tf.expand_dims_1[0][0]']       \n",
      "                                 1)                                                               \n",
      "                                                                                                  \n",
      " time_distributed_5 (TimeDistri  (None, 1, 48, 48, 6  294976     ['tf.expand_dims[0][0]']         \n",
      " buted)                         4)                                                                \n",
      "                                                                                                  \n",
      " time_distributed_8 (TimeDistri  (None, 1, 100, 100,  2305       ['time_distributed_7[0][0]']     \n",
      " buted)                          1)                                                               \n",
      "                                                                                                  \n",
      " tf.math.subtract (TFOpLambda)  (None, 1, 100, 100,  0           ['time_distributed_3[0][0]',     \n",
      "                                 1)                               'tf.expand_dims_2[0][0]']       \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_1 (Sl  (None, 100, 100)    0           ['input_1[0][0]']                \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " time_distributed_6 (TimeDistri  (None, 1, 100, 100,  2305       ['time_distributed_5[0][0]']     \n",
      " buted)                          1)                                                               \n",
      "                                                                                                  \n",
      " tf.concat (TFOpLambda)         (None, 1, 100, 100,  0           ['time_distributed_8[0][0]',     \n",
      "                                 2)                               'tf.math.subtract[0][0]']       \n",
      "                                                                                                  \n",
      " tf.expand_dims_3 (TFOpLambda)  (None, 1, 100, 100)  0           ['tf.__operators__.getitem_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " time_distributed_4 (TimeDistri  (None, 1, 100, 100,  37         ['time_distributed_3[0][0]']     \n",
      " buted)                          1)                                                               \n",
      "                                                                                                  \n",
      " tf.math.multiply (TFOpLambda)  (None, 1, 100, 100,  0           ['time_distributed_3[0][0]',     \n",
      "                                 1)                               'time_distributed_6[0][0]']     \n",
      "                                                                                                  \n",
      " time_distributed_9 (TimeDistri  (None, 1, 100, 100,  73         ['tf.concat[0][0]']              \n",
      " buted)                          1)                                                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_4 (TFOpLambda)  (None, 1, 100, 100,  0           ['tf.expand_dims_3[0][0]']       \n",
      "                                 1)                                                               \n",
      "                                                                                                  \n",
      " tf.concat_1 (TFOpLambda)       (None, 1, 100, 100,  0           ['time_distributed_3[0][0]',     \n",
      "                                 5)                               'time_distributed_4[0][0]',     \n",
      "                                                                  'tf.math.multiply[0][0]',       \n",
      "                                                                  'time_distributed_9[0][0]',     \n",
      "                                                                  'tf.expand_dims_4[0][0]']       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,179,694\n",
      "Trainable params: 4,179,694\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Model(inp, out)\n",
    "model.compile(\n",
    "    # loss=keras.losses.MeanSquaredError(reduction=\"auto\", name=\"mean_squared_error\"),\n",
    "    loss=PINN_loss(),\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-4))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "X5Q7hmIAAsiX",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-16 01:03:24.972258: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 3000000000 exceeds 10% of free system memory.\n",
      "2022-07-16 01:03:26.732380: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 3000000000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-16 01:03:30.020315: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:903] layout failed: INVALID_ARGUMENT: MutableGraphView::SortTopologically error: detected edge(s) creating cycle(s) {'Func/gradient_tape/model/conv_lstm2d/while/model/conv_lstm2d/while_grad/body/_189/input/_551' -> 'gradient_tape/model/conv_lstm2d/while/model/conv_lstm2d/while_grad/body/_189/gradient_tape/model/conv_lstm2d/while/gradients/AddN', 'model/conv_lstm2d/while/body/_1/model/conv_lstm2d/while/clip_by_value' -> 'model/conv_lstm2d/while/body/_1/model/conv_lstm2d/while/mul_3', 'model/conv_lstm2d/while/body/_1/model/conv_lstm2d/while/convolution_6' -> 'model/conv_lstm2d/while/body/_1/model/conv_lstm2d/while/add_4', 'model/conv_lstm2d/while/body/_1/model/conv_lstm2d/while/clip_by_value_2' -> 'model/conv_lstm2d/while/body/_1/model/conv_lstm2d/while/mul_5', 'model/conv_lstm2d/while/body/_1/model/conv_lstm2d/while/mul_2' -> 'model/conv_lstm2d/while/body/_1/model/conv_lstm2d/while/add_5'}.\n",
      "2022-07-16 01:03:30.790576: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-07-16 01:03:30.877151: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8303\n",
      "2022-07-16 01:03:31.627668: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - ETA: 0s - loss: 0.2304"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-16 01:04:17.403635: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:903] layout failed: INVALID_ARGUMENT: MutableGraphView::SortTopologically error: detected edge(s) creating cycle(s) {'model/conv_lstm2d/while/body/_1/model/conv_lstm2d/while/convolution_7' -> 'model/conv_lstm2d/while/body/_1/model/conv_lstm2d/while/add_6', 'model/conv_lstm2d/while/body/_1/model/conv_lstm2d/while/Relu_1' -> 'model/conv_lstm2d/while/body/_1/model/conv_lstm2d/while/mul_5', 'Func/model/conv_lstm2d/while/body/_1/input/_68' -> 'model/conv_lstm2d/while/body/_1/model/conv_lstm2d/while/mul_2'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 54s 33ms/step - loss: 0.2304 - val_loss: 0.1504 - lr: 1.0000e-04\n",
      "Epoch 2/1000\n",
      "1500/1500 [==============================] - 49s 32ms/step - loss: 0.1164 - val_loss: 0.1140 - lr: 1.0000e-04\n",
      "Epoch 3/1000\n",
      "1500/1500 [==============================] - 49s 32ms/step - loss: 0.0835 - val_loss: 0.0910 - lr: 1.0000e-04\n",
      "Epoch 4/1000\n",
      "1500/1500 [==============================] - 49s 32ms/step - loss: 0.0739 - val_loss: 0.0692 - lr: 1.0000e-04\n",
      "Epoch 5/1000\n",
      "1500/1500 [==============================] - 49s 33ms/step - loss: 0.0670 - val_loss: 0.0658 - lr: 1.0000e-04\n",
      "Epoch 6/1000\n",
      "1500/1500 [==============================] - 49s 33ms/step - loss: 0.0638 - val_loss: 0.0604 - lr: 1.0000e-04\n",
      "Epoch 7/1000\n",
      "1500/1500 [==============================] - 49s 33ms/step - loss: 0.0617 - val_loss: 0.0601 - lr: 1.0000e-04\n",
      "Epoch 8/1000\n",
      "1500/1500 [==============================] - 48s 32ms/step - loss: 0.0581 - val_loss: 0.0575 - lr: 1.0000e-04\n",
      "Epoch 9/1000\n",
      "1500/1500 [==============================] - 49s 32ms/step - loss: 0.0556 - val_loss: 0.0541 - lr: 1.0000e-04\n",
      "Epoch 10/1000\n",
      "1500/1500 [==============================] - 48s 32ms/step - loss: 0.0517 - val_loss: 0.0513 - lr: 1.0000e-04\n",
      "Epoch 11/1000\n",
      "1500/1500 [==============================] - 48s 32ms/step - loss: 0.0506 - val_loss: 0.0500 - lr: 1.0000e-04\n",
      "Epoch 12/1000\n",
      "1500/1500 [==============================] - 48s 32ms/step - loss: 0.0494 - val_loss: 0.0505 - lr: 1.0000e-04\n",
      "Epoch 13/1000\n",
      "1500/1500 [==============================] - 49s 33ms/step - loss: 0.0469 - val_loss: 0.0487 - lr: 1.0000e-04\n",
      "Epoch 14/1000\n",
      "1500/1500 [==============================] - 49s 32ms/step - loss: 0.0456 - val_loss: 0.0497 - lr: 1.0000e-04\n",
      "Epoch 15/1000\n",
      "1500/1500 [==============================] - 49s 32ms/step - loss: 0.0437 - val_loss: 0.0474 - lr: 1.0000e-04\n",
      "Epoch 16/1000\n",
      "1500/1500 [==============================] - 48s 32ms/step - loss: 0.0428 - val_loss: 0.0502 - lr: 1.0000e-04\n",
      "Epoch 17/1000\n",
      "1500/1500 [==============================] - 49s 32ms/step - loss: 0.0423 - val_loss: 0.0447 - lr: 1.0000e-04\n",
      "Epoch 18/1000\n",
      "1500/1500 [==============================] - 49s 33ms/step - loss: 0.0409 - val_loss: 0.0447 - lr: 1.0000e-04\n",
      "Epoch 19/1000\n",
      "1500/1500 [==============================] - 49s 33ms/step - loss: 0.0392 - val_loss: 0.0431 - lr: 1.0000e-04\n",
      "Epoch 20/1000\n",
      "1500/1500 [==============================] - 49s 33ms/step - loss: 0.0391 - val_loss: 0.0450 - lr: 1.0000e-04\n",
      "Epoch 21/1000\n",
      "1500/1500 [==============================] - 49s 33ms/step - loss: 0.0384 - val_loss: 0.0433 - lr: 1.0000e-04\n",
      "Epoch 22/1000\n",
      "1500/1500 [==============================] - 49s 33ms/step - loss: 0.0379 - val_loss: 0.0392 - lr: 1.0000e-04\n",
      "Epoch 23/1000\n",
      "1500/1500 [==============================] - 49s 32ms/step - loss: 0.0378 - val_loss: 0.0416 - lr: 1.0000e-04\n",
      "Epoch 24/1000\n",
      "1500/1500 [==============================] - 49s 33ms/step - loss: 0.0362 - val_loss: 0.0388 - lr: 1.0000e-04\n",
      "Epoch 25/1000\n",
      "1500/1500 [==============================] - 49s 33ms/step - loss: 0.0355 - val_loss: 0.0448 - lr: 1.0000e-04\n",
      "Epoch 26/1000\n",
      "1500/1500 [==============================] - 49s 32ms/step - loss: 0.0349 - val_loss: 0.0376 - lr: 1.0000e-04\n",
      "Epoch 27/1000\n",
      "1500/1500 [==============================] - 49s 33ms/step - loss: 0.0347 - val_loss: 0.0444 - lr: 1.0000e-04\n",
      "Epoch 28/1000\n",
      "1500/1500 [==============================] - 49s 33ms/step - loss: 0.0336 - val_loss: 0.0373 - lr: 1.0000e-04\n",
      "Epoch 29/1000\n",
      "1500/1500 [==============================] - 49s 33ms/step - loss: 0.0336 - val_loss: 0.0382 - lr: 1.0000e-04\n",
      "Epoch 30/1000\n",
      "1500/1500 [==============================] - 49s 33ms/step - loss: 0.0328 - val_loss: 0.0362 - lr: 1.0000e-04\n",
      "Epoch 31/1000\n",
      "1500/1500 [==============================] - 49s 32ms/step - loss: 0.0323 - val_loss: 0.0347 - lr: 1.0000e-04\n",
      "Epoch 32/1000\n",
      "1500/1500 [==============================] - 48s 32ms/step - loss: 0.0326 - val_loss: 0.0360 - lr: 1.0000e-04\n",
      "Epoch 33/1000\n",
      "1500/1500 [==============================] - 49s 33ms/step - loss: 0.0312 - val_loss: 0.0350 - lr: 1.0000e-04\n",
      "Epoch 34/1000\n",
      "1500/1500 [==============================] - 49s 32ms/step - loss: 0.0306 - val_loss: 0.0384 - lr: 1.0000e-04\n",
      "Epoch 35/1000\n",
      "1500/1500 [==============================] - 49s 32ms/step - loss: 0.0306 - val_loss: 0.0344 - lr: 1.0000e-04\n",
      "Epoch 36/1000\n",
      "1500/1500 [==============================] - 48s 32ms/step - loss: 0.0301 - val_loss: 0.0335 - lr: 1.0000e-04\n",
      "Epoch 37/1000\n",
      "1500/1500 [==============================] - 48s 32ms/step - loss: 0.0293 - val_loss: 0.0346 - lr: 1.0000e-04\n",
      "Epoch 38/1000\n",
      "1500/1500 [==============================] - 49s 32ms/step - loss: 0.0297 - val_loss: 0.0341 - lr: 1.0000e-04\n",
      "Epoch 39/1000\n",
      "1500/1500 [==============================] - 48s 32ms/step - loss: 0.0285 - val_loss: 0.0322 - lr: 1.0000e-04\n",
      "Epoch 40/1000\n",
      "1500/1500 [==============================] - 49s 32ms/step - loss: 0.0290 - val_loss: 0.0325 - lr: 1.0000e-04\n",
      "Epoch 41/1000\n",
      "1500/1500 [==============================] - 49s 32ms/step - loss: 0.0280 - val_loss: 0.0343 - lr: 1.0000e-04\n",
      "Epoch 42/1000\n",
      "1500/1500 [==============================] - 49s 32ms/step - loss: 0.0272 - val_loss: 0.0316 - lr: 1.0000e-04\n",
      "Epoch 43/1000\n",
      "1500/1500 [==============================] - 49s 32ms/step - loss: 0.0266 - val_loss: 0.0383 - lr: 1.0000e-04\n",
      "Epoch 44/1000\n",
      "1500/1500 [==============================] - 49s 32ms/step - loss: 0.0274 - val_loss: 0.0322 - lr: 1.0000e-04\n",
      "Epoch 45/1000\n",
      "1500/1500 [==============================] - 49s 32ms/step - loss: 0.0272 - val_loss: 0.0312 - lr: 1.0000e-04\n",
      "Epoch 46/1000\n",
      "1500/1500 [==============================] - 49s 33ms/step - loss: 0.0266 - val_loss: 0.0349 - lr: 1.0000e-04\n",
      "Epoch 47/1000\n",
      "1500/1500 [==============================] - 48s 32ms/step - loss: 0.0261 - val_loss: 0.0318 - lr: 1.0000e-04\n",
      "Epoch 48/1000\n",
      "1500/1500 [==============================] - 49s 32ms/step - loss: 0.0266 - val_loss: 0.0322 - lr: 1.0000e-04\n",
      "Epoch 49/1000\n",
      "1500/1500 [==============================] - 49s 32ms/step - loss: 0.0254 - val_loss: 0.0308 - lr: 1.0000e-04\n",
      "Epoch 50/1000\n",
      "1500/1500 [==============================] - 48s 32ms/step - loss: 0.0250 - val_loss: 0.0308 - lr: 1.0000e-04\n",
      "Epoch 51/1000\n",
      "1500/1500 [==============================] - 49s 32ms/step - loss: 0.0253 - val_loss: 0.0319 - lr: 1.0000e-04\n",
      "Epoch 52/1000\n",
      "1500/1500 [==============================] - 48s 32ms/step - loss: 0.0249 - val_loss: 0.0318 - lr: 1.0000e-04\n",
      "Epoch 53/1000\n",
      "1500/1500 [==============================] - 48s 32ms/step - loss: 0.0250 - val_loss: 0.0313 - lr: 1.0000e-04\n",
      "Epoch 54/1000\n",
      "1500/1500 [==============================] - 49s 32ms/step - loss: 0.0240 - val_loss: 0.0318 - lr: 1.0000e-04\n",
      "Epoch 55/1000\n",
      "1500/1500 [==============================] - 49s 32ms/step - loss: 0.0243 - val_loss: 0.0294 - lr: 1.0000e-04\n",
      "Epoch 56/1000\n",
      "1500/1500 [==============================] - 48s 32ms/step - loss: 0.0241 - val_loss: 0.0306 - lr: 1.0000e-04\n",
      "Epoch 57/1000\n",
      "1500/1500 [==============================] - 48s 32ms/step - loss: 0.0242 - val_loss: 0.0311 - lr: 1.0000e-04\n",
      "Epoch 58/1000\n",
      "1500/1500 [==============================] - 49s 32ms/step - loss: 0.0232 - val_loss: 0.0301 - lr: 1.0000e-04\n",
      "Epoch 59/1000\n",
      "1500/1500 [==============================] - 49s 32ms/step - loss: 0.0233 - val_loss: 0.0294 - lr: 1.0000e-04\n",
      "Epoch 60/1000\n",
      "1500/1500 [==============================] - 49s 33ms/step - loss: 0.0234 - val_loss: 0.0302 - lr: 1.0000e-04\n",
      "Epoch 61/1000\n",
      "1500/1500 [==============================] - 48s 32ms/step - loss: 0.0230 - val_loss: 0.0312 - lr: 1.0000e-04\n",
      "Epoch 62/1000\n",
      "1500/1500 [==============================] - 48s 32ms/step - loss: 0.0226 - val_loss: 0.0297 - lr: 1.0000e-04\n",
      "Epoch 63/1000\n",
      "1500/1500 [==============================] - 48s 32ms/step - loss: 0.0230 - val_loss: 0.0296 - lr: 1.0000e-04\n",
      "Epoch 64/1000\n",
      "1500/1500 [==============================] - 48s 32ms/step - loss: 0.0226 - val_loss: 0.0298 - lr: 1.0000e-04\n",
      "Epoch 65/1000\n",
      "1500/1500 [==============================] - 48s 32ms/step - loss: 0.0227 - val_loss: 0.0298 - lr: 1.0000e-04\n",
      "Epoch 66/1000\n",
      "1500/1500 [==============================] - 49s 32ms/step - loss: 0.0222 - val_loss: 0.0291 - lr: 1.0000e-04\n",
      "Epoch 67/1000\n",
      "1500/1500 [==============================] - 49s 32ms/step - loss: 0.0222 - val_loss: 0.0286 - lr: 1.0000e-04\n",
      "Epoch 68/1000\n",
      "1500/1500 [==============================] - 48s 32ms/step - loss: 0.0220 - val_loss: 0.0299 - lr: 1.0000e-04\n",
      "Epoch 69/1000\n",
      "1500/1500 [==============================] - 48s 32ms/step - loss: 0.0222 - val_loss: 0.0299 - lr: 1.0000e-04\n",
      "Epoch 70/1000\n",
      "1500/1500 [==============================] - 49s 32ms/step - loss: 0.0218 - val_loss: 0.0293 - lr: 1.0000e-04\n",
      "Epoch 71/1000\n",
      "1500/1500 [==============================] - 48s 32ms/step - loss: 0.0216 - val_loss: 0.0284 - lr: 1.0000e-04\n",
      "Epoch 72/1000\n",
      "1500/1500 [==============================] - 48s 32ms/step - loss: 0.0220 - val_loss: 0.0298 - lr: 1.0000e-04\n",
      "Epoch 73/1000\n",
      "1500/1500 [==============================] - 49s 32ms/step - loss: 0.0218 - val_loss: 0.0285 - lr: 1.0000e-04\n",
      "Epoch 74/1000\n",
      "1500/1500 [==============================] - 48s 32ms/step - loss: 0.0209 - val_loss: 0.0304 - lr: 1.0000e-04\n",
      "Epoch 75/1000\n",
      "1500/1500 [==============================] - 48s 32ms/step - loss: 0.0208 - val_loss: 0.0296 - lr: 1.0000e-04\n",
      "Epoch 76/1000\n",
      "1500/1500 [==============================] - 49s 32ms/step - loss: 0.0200 - val_loss: 0.0292 - lr: 1.0000e-04\n",
      "Epoch 77/1000\n",
      "1500/1500 [==============================] - 49s 32ms/step - loss: 0.0206 - val_loss: 0.0291 - lr: 1.0000e-04\n",
      "Epoch 78/1000\n",
      "1500/1500 [==============================] - 48s 32ms/step - loss: 0.0202 - val_loss: 0.0290 - lr: 1.0000e-04\n",
      "Epoch 79/1000\n",
      "1500/1500 [==============================] - 48s 32ms/step - loss: 0.0204 - val_loss: 0.0285 - lr: 1.0000e-04\n",
      "Epoch 80/1000\n",
      "1500/1500 [==============================] - 49s 32ms/step - loss: 0.0206 - val_loss: 0.0302 - lr: 1.0000e-04\n",
      "Epoch 81/1000\n",
      "1500/1500 [==============================] - 49s 32ms/step - loss: 0.0197 - val_loss: 0.0296 - lr: 1.0000e-04\n",
      "Epoch 82/1000\n",
      "1500/1500 [==============================] - 49s 33ms/step - loss: 0.0201 - val_loss: 0.0314 - lr: 1.0000e-04\n",
      "Epoch 83/1000\n",
      "1500/1500 [==============================] - 48s 32ms/step - loss: 0.0203 - val_loss: 0.0289 - lr: 1.0000e-04\n",
      "Epoch 84/1000\n",
      "1500/1500 [==============================] - 49s 32ms/step - loss: 0.0199 - val_loss: 0.0295 - lr: 1.0000e-04\n",
      "Epoch 85/1000\n",
      "1500/1500 [==============================] - 48s 32ms/step - loss: 0.0195 - val_loss: 0.0285 - lr: 1.0000e-04\n",
      "Epoch 86/1000\n",
      "1500/1500 [==============================] - 48s 32ms/step - loss: 0.0192 - val_loss: 0.0284 - lr: 1.0000e-04\n",
      "Epoch 87/1000\n",
      "1500/1500 [==============================] - 49s 32ms/step - loss: 0.0184 - val_loss: 0.0282 - lr: 1.0000e-05\n",
      "Epoch 88/1000\n",
      "1500/1500 [==============================] - 49s 32ms/step - loss: 0.0170 - val_loss: 0.0282 - lr: 1.0000e-05\n",
      "Epoch 89/1000\n",
      "1500/1500 [==============================] - 49s 32ms/step - loss: 0.0175 - val_loss: 0.0290 - lr: 1.0000e-05\n",
      "Epoch 90/1000\n",
      "1500/1500 [==============================] - 48s 32ms/step - loss: 0.0170 - val_loss: 0.0288 - lr: 1.0000e-05\n",
      "Epoch 91/1000\n",
      "1500/1500 [==============================] - 49s 32ms/step - loss: 0.0172 - val_loss: 0.0288 - lr: 1.0000e-05\n",
      "Epoch 92/1000\n",
      "1500/1500 [==============================] - 49s 32ms/step - loss: 0.0171 - val_loss: 0.0283 - lr: 1.0000e-05\n",
      "Epoch 93/1000\n",
      "1500/1500 [==============================] - 49s 32ms/step - loss: 0.0174 - val_loss: 0.0290 - lr: 1.0000e-05\n",
      "Epoch 94/1000\n",
      "1500/1500 [==============================] - 49s 32ms/step - loss: 0.0165 - val_loss: 0.0289 - lr: 1.0000e-05\n",
      "Epoch 95/1000\n",
      "1500/1500 [==============================] - 48s 32ms/step - loss: 0.0171 - val_loss: 0.0292 - lr: 1.0000e-05\n",
      "Epoch 96/1000\n",
      "1500/1500 [==============================] - 49s 32ms/step - loss: 0.0169 - val_loss: 0.0288 - lr: 1.0000e-05\n",
      "Epoch 97/1000\n",
      "1500/1500 [==============================] - 49s 32ms/step - loss: 0.0166 - val_loss: 0.0290 - lr: 1.0000e-05\n",
      "Epoch 98/1000\n",
      "1500/1500 [==============================] - 49s 32ms/step - loss: 0.0169 - val_loss: 0.0292 - lr: 1.0000e-05\n",
      "Epoch 99/1000\n",
      "1500/1500 [==============================] - 49s 32ms/step - loss: 0.0169 - val_loss: 0.0291 - lr: 1.0000e-05\n",
      "Epoch 100/1000\n",
      "1500/1500 [==============================] - 48s 32ms/step - loss: 0.0166 - val_loss: 0.0291 - lr: 1.0000e-05\n",
      "Epoch 101/1000\n",
      "1500/1500 [==============================] - 49s 32ms/step - loss: 0.0167 - val_loss: 0.0292 - lr: 1.0000e-05\n",
      "Epoch 102/1000\n",
      "1500/1500 [==============================] - 48s 32ms/step - loss: 0.0165 - val_loss: 0.0291 - lr: 1.0000e-05\n",
      "Epoch 103/1000\n",
      "1500/1500 [==============================] - 48s 32ms/step - loss: 0.0165 - val_loss: 0.0291 - lr: 1.0000e-06\n",
      "Epoch 104/1000\n",
      "1500/1500 [==============================] - 48s 32ms/step - loss: 0.0164 - val_loss: 0.0290 - lr: 1.0000e-06\n",
      "Epoch 105/1000\n",
      "1500/1500 [==============================] - 49s 32ms/step - loss: 0.0160 - val_loss: 0.0288 - lr: 1.0000e-06\n",
      "Epoch 106/1000\n",
      "1500/1500 [==============================] - 48s 32ms/step - loss: 0.0164 - val_loss: 0.0290 - lr: 1.0000e-06\n",
      "Epoch 107/1000\n",
      "1500/1500 [==============================] - 49s 32ms/step - loss: 0.0168 - val_loss: 0.0289 - lr: 1.0000e-06\n",
      "Epoch 108/1000\n",
      "1500/1500 [==============================] - 49s 32ms/step - loss: 0.0165 - val_loss: 0.0290 - lr: 1.0000e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./saved_model/model_conv_lstm_test_17/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./saved_model/model_conv_lstm_test_17/assets\n"
     ]
    }
   ],
   "source": [
    "class CustomSaver(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch == 10:  # or save after some epoch, each k-th epoch etc.\n",
    "            self.model.save(\"model_{}.hd5\".format(epoch))\n",
    "            \n",
    "test = False\n",
    "\n",
    "if test == True:\n",
    "    model = keras.models.load_model('./saved_model/model_conv_lstm_test_17')\n",
    "else: \n",
    "    early_stopping = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=20)\n",
    "    reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=15)\n",
    "    epochs = 1000\n",
    "    batch_size = 2\n",
    "    # saver = CustomSaver()\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        model.fit(\n",
    "            x_train,\n",
    "            y_train,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            validation_data=(x_val, y_val),\n",
    "            # callbacks=[saver,early_stopping, reduce_lr],\n",
    "            callbacks=[early_stopping, reduce_lr],\n",
    "        )\n",
    "\n",
    "    model.save('./saved_model/model_conv_lstm_test_17') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/kuanrenqian/Documents/NeuronGrowthML/tensorflow_version/NeuronGrowthML_main_test_17.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kuanrenqian/Documents/NeuronGrowthML/tensorflow_version/NeuronGrowthML_main_test_17.ipynb#ch0000008?line=12'>13</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39msqrt(np\u001b[39m.\u001b[39msum(np\u001b[39m.\u001b[39msquare(pred\u001b[39m-\u001b[39mgoal)\u001b[39m/\u001b[39m(pred\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\u001b[39m*\u001b[39mpred\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m])))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kuanrenqian/Documents/NeuronGrowthML/tensorflow_version/NeuronGrowthML_main_test_17.ipynb#ch0000008?line=14'>15</a>\u001b[0m \u001b[39m# generating intial 5 frame input for predictions\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/kuanrenqian/Documents/NeuronGrowthML/tensorflow_version/NeuronGrowthML_main_test_17.ipynb#ch0000008?line=15'>16</a>\u001b[0m rand_case \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandint((\u001b[39mlen\u001b[39m(val_dataset)))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kuanrenqian/Documents/NeuronGrowthML/tensorflow_version/NeuronGrowthML_main_test_17.ipynb#ch0000008?line=16'>17</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mRandom case: \u001b[39m\u001b[39m{\u001b[39;00mrand_case\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kuanrenqian/Documents/NeuronGrowthML/tensorflow_version/NeuronGrowthML_main_test_17.ipynb#ch0000008?line=17'>18</a>\u001b[0m example_x \u001b[39m=\u001b[39m val_dataset[rand_case,:,:,:,\u001b[39m0\u001b[39m:\u001b[39m5\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "import matlab.engine\n",
    "import copy\n",
    "eng = matlab.engine.start_matlab()\n",
    "\n",
    "def push(input, cutoff):\n",
    "    max_input = np.amax(input)\n",
    "    output = copy.copy(input)\n",
    "    output[input>cutoff*max_input] = 1\n",
    "    output[input<=cutoff*max_input] = 0\n",
    "    return output\n",
    "\n",
    "def get_mre(pred,goal):\n",
    "    return np.sqrt(np.sum(np.square(pred-goal)/(pred.shape[0]*pred.shape[1])))\n",
    "\n",
    "# generating intial 5 frame input for predictions\n",
    "rand_case = np.random.randint((len(val_dataset)))\n",
    "print(f\"Random case: {rand_case}\")\n",
    "example_x = val_dataset[rand_case,:,:,:,0:5]\n",
    "x_in = np.expand_dims(example_x[0:5,...],axis=0)\n",
    "\n",
    "for i in tqdm(range(64), desc=\"Plotting and Saving figures ...\"):\n",
    "    new_prediction = model.predict(x_in,verbose=\"0\")\n",
    "    phi = new_prediction[0,0,:,:,0]\n",
    "    tips = np.round(eng.generate_tips_for_python(matlab.double(np.array(phi).astype('float64')),5000))\n",
    "    tips[:,0:5] = 0 # issue in matlab code, need to be fixed later\n",
    "    # tips = push(new_prediction[0,0,:,:,1],0.5)\n",
    "    # tips = new_prediction[0,0,:,:,1]\n",
    "    tub = new_prediction[0,0,:,:,2]\n",
    "    tempr = new_prediction[0,0,:,:,3]\n",
    "    theta = new_prediction[0,0,:,:,4]\n",
    "\n",
    "    iter = (i+1)*500+2500\n",
    "    plt.figure(figsize=(25, 12), dpi=60)\n",
    "    plt.gcf().set_facecolor(\"white\")\n",
    "    plt.subplot(3,4,1)\n",
    "    plt.imshow(phi, cmap='jet')\n",
    "    plt.colorbar()\n",
    "    plt.title(f\"Pred Phi at iter {iter}\", fontweight='bold')\n",
    "    plt.subplot(3,4,2)\n",
    "    plt.imshow(tips, cmap='jet')\n",
    "    plt.colorbar()\n",
    "    plt.title(f\"Pred Tips at iter {iter}\", fontweight='bold')\n",
    "    plt.subplot(3,4,3)\n",
    "    plt.imshow(tub, cmap='jet')\n",
    "    plt.colorbar()\n",
    "    plt.title(f\"Pred Tubulin at iter {iter}\", fontweight='bold')\n",
    "    plt.subplot(3,4,4)\n",
    "    plt.imshow(tempr, cmap='jet')\n",
    "    plt.colorbar()\n",
    "    plt.title(f\"Pred Tempr at iter {iter}\", fontweight='bold')\n",
    "\n",
    "    plt.subplot(3,4,5)\n",
    "    plt.imshow(example_x[i+5,:,:,0], cmap='jet')\n",
    "    plt.colorbar()\n",
    "    plt.title(f\"Ground Truth phi at iter {iter}\", fontweight='bold')\n",
    "    plt.subplot(3,4,6)\n",
    "    plt.imshow(example_x[i+5,:,:,1], cmap='jet')\n",
    "    plt.colorbar()\n",
    "    plt.title(f\"Ground Truth Tips at iter {iter}\", fontweight='bold')\n",
    "    plt.subplot(3,4,7)\n",
    "    plt.imshow(example_x[i+5,:,:,2], cmap='jet')\n",
    "    plt.colorbar()\n",
    "    plt.title(f\"Ground Truth Tubulin at iter {iter}\", fontweight='bold')\n",
    "    plt.subplot(3,4,8)\n",
    "    plt.imshow(example_x[i+5,:,:,3], cmap='jet')\n",
    "    plt.colorbar()\n",
    "    plt.title(f\"Ground Truth Tempr at iter {iter}\", fontweight='bold')\n",
    "\n",
    "    mre_phi = get_mre(phi,example_x[i+5,:,:,0])\n",
    "    mre_tips = get_mre(tips,example_x[i+5,:,:,1])\n",
    "    mre_tempr = get_mre(tempr,example_x[i+5,:,:,2])\n",
    "    mre_tub = get_mre(tub,example_x[i+5,:,:,3])\n",
    "    mre_theta = get_mre(theta,example_x[i+5,:,:,4])\n",
    "\n",
    "    plt.subplot(3,4,9)\n",
    "    plt.imshow(phi-example_x[i+5,:,:,0], cmap='jet')\n",
    "    plt.colorbar()\n",
    "    plt.title(f\"Absolute phi at iter {iter}\", fontweight='bold')\n",
    "    plt.xlabel(f\"MRE error: {mre_phi}\")\n",
    "    plt.subplot(3,4,10)\n",
    "    plt.imshow(tips-example_x[i+5,:,:,1], cmap='jet')\n",
    "    plt.colorbar()\n",
    "    plt.title(f\"Absolute Tips at iter {iter}\", fontweight='bold')\n",
    "    plt.xlabel(f\"MRE error: {mre_tips}\")\n",
    "    plt.subplot(3,4,11)\n",
    "    plt.imshow(tub-example_x[i+5,:,:,2], cmap='jet')\n",
    "    plt.colorbar()\n",
    "    plt.title(f\"Absolute Tubulin at iter {iter}\", fontweight='bold')\n",
    "    plt.xlabel(f\"MRE error: {mre_tempr}\")\n",
    "    plt.subplot(3,4,12)\n",
    "    plt.imshow(tempr-example_x[i+5,:,:,3], cmap='jet')\n",
    "    plt.colorbar()\n",
    "    plt.title(f\"Absolute Tempr at iter {iter}\", fontweight='bold')\n",
    "    plt.xlabel(f\"MRE error: {mre_tub}\")\n",
    "\n",
    "    plt.savefig(f\"temp/gif/{iter:05d}.png\")\n",
    "    # plt.show()\n",
    "\n",
    "    # update prediction for later use as input\n",
    "    tmp_var = copy.copy(new_prediction)\n",
    "    tmp_var[0,-1,:,:,1] = tips\n",
    "    tmp = np.zeros(x_in.shape)\n",
    "    tmp[0,0,...] = x_in[0,1,...]\n",
    "    tmp[0,1,...] = x_in[0,2,...]\n",
    "    tmp[0,2,...] = x_in[0,3,...]\n",
    "    tmp[0,3,...] = x_in[0,4,...]\n",
    "    tmp[0,4,:,:,:] = tmp_var\n",
    "\n",
    "    x_in = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "conv_lstm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "3ef443919b3ee49497caa0ae1c9dcd325c651b678aa5033135de9f40fcfd4216"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
